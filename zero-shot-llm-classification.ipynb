{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5707eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting groq\n",
      "  Downloading groq-0.32.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from pytesseract) (11.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from groq) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from groq) (2.10.3)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from groq) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\pawanmagapalli\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.27.1)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading groq-0.32.0-py3-none-any.whl (135 kB)\n",
      "Installing collected packages: pytesseract, PyPDF2, pdf2image, groq\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [PyPDF2]\n",
      "   ---------- ----------------------------- 1/4 [PyPDF2]\n",
      "   ---------- ----------------------------- 1/4 [PyPDF2]\n",
      "   ---------- ----------------------------- 1/4 [PyPDF2]\n",
      "   ------------------------------ --------- 3/4 [groq]\n",
      "   ------------------------------ --------- 3/4 [groq]\n",
      "   ------------------------------ --------- 3/4 [groq]\n",
      "   ------------------------------ --------- 3/4 [groq]\n",
      "   ------------------------------ --------- 3/4 [groq]\n",
      "   ------------------------------ --------- 3/4 [groq]\n",
      "   ---------------------------------------- 4/4 [groq]\n",
      "\n",
      "Successfully installed PyPDF2-3.0.1 groq-0.32.0 pdf2image-1.17.0 pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 pytesseract pdf2image groq\n",
    "# Note: For Windows, you might need to install poppler and tesseract separately\n",
    "# Download poppler from: https://github.com/oschwartz10612/poppler-windows/releases/\n",
    "# Download tesseract from: https://github.com/UB-Mannheim/tesseract/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0813ce75",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be929fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "from groq import Groq\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c415d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model llama-3.1-8b-instant failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': ...\n",
      "Model mixtral-8x7b-32768 failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': ...\n",
      "Model gemma-7b-it failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': ...\n",
      "Model llama3-8b-8192 failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': ...\n",
      "✗ No working models found\n"
     ]
    }
   ],
   "source": [
    "# Set your Groq API key as environment variable first\n",
    "# Make sure to replace this with your actual API key\n",
    "os.environ['GROQ_API_KEY'] = \"-----\"\n",
    "\n",
    "try:\n",
    "    client = Groq(api_key=os.environ.get(\"GROQ_API_KEY\"))\n",
    "    # Test with multiple models to find a working one\n",
    "    models_to_try = [\n",
    "        \"llama-3.1-8b-instant\",\n",
    "        \"mixtral-8x7b-32768\", \n",
    "        \"gemma-7b-it\",\n",
    "        \"llama3-8b-8192\"\n",
    "    ]\n",
    "    \n",
    "    working_model = None\n",
    "    for model in models_to_try:\n",
    "        try:\n",
    "            test_response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                max_tokens=5\n",
    "            )\n",
    "            working_model = model\n",
    "            print(f\"✓ Groq API connection successful with model: {model}\")\n",
    "            break\n",
    "        except Exception as model_error:\n",
    "            print(f\"Model {model} failed: {str(model_error)[:100]}...\")\n",
    "            continue\n",
    "    \n",
    "    if not working_model:\n",
    "        print(\"✗ No working models found\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Groq API connection failed: {e}\")\n",
    "    print(\"Please check your API key and internet connection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e635c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text using hybrid approach...\n",
      "Page 1: Using OCR (normal extraction found 0 characters)\n",
      "  OCR failed for page 1: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Page 2: Using OCR (normal extraction found 0 characters)\n",
      "  OCR failed for page 2: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Page 3: Using OCR (normal extraction found 0 characters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(60, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(67, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(73, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(85, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(91, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(97, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(103, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(109, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(115, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(123, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(129, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(135, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(141, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(147, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(153, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(159, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(165, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(171, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(177, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(185, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(191, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(197, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(203, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(209, 0, 2116820886800)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OCR failed for page 3: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Page 4: Normal extraction found 1333 characters\n",
      "Page 5: Normal extraction found 1131 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(215, 0, 2116820886800)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 6: Normal extraction found 1319 characters\n",
      "Page 7: Using OCR (normal extraction found 0 characters)\n",
      "  OCR failed for page 7: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Page 8: Using OCR (normal extraction found 0 characters)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(245, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(251, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(257, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(263, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(269, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(275, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(281, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(287, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(293, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(301, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(307, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(313, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(319, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(325, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(331, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(337, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 2116820886800)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OCR failed for page 8: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Page 9: Normal extraction found 1370 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unknown widths : \n",
      "[0, IndirectObject(343, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(349, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(355, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(361, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(369, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(375, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(381, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(79, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(387, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(393, 0, 2116820886800)]\n",
      "unknown widths : \n",
      "[0, IndirectObject(399, 0, 2116820886800)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 10: Normal extraction found 1137 characters\n",
      "Page 11: Normal extraction found 1341 characters\n",
      "Page 12: Using OCR (normal extraction found 0 characters)\n",
      "  OCR failed for page 12: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Page 13: Using OCR (normal extraction found 0 characters)\n",
      "  OCR failed for page 13: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "\n",
      "Total pages processed: 13\n",
      "\n",
      "Content summary:\n",
      "Page 1: 2 words\n",
      "  Status: Empty or no extractable text\n",
      "Page 2: 2 words\n",
      "  Status: Empty or no extractable text\n",
      "Page 3: 2 words\n",
      "  Status: Empty or no extractable text\n",
      "Page 4: 202 words\n",
      "  Preview: W-2 Form W-2 Wage & Tax Statement 2023 Scan QR code to go to TurboTax and Import your  W-2 informatl...\n",
      "Page 5: 184 words\n",
      "  Preview: Form W-2 Wage & Tax Statement 2023  Copy 2 -To Be Flied With Employee's State, City, or Local Income...\n",
      "Page 6: 216 words\n",
      "  Preview: Form W-2 Wage & Tax Statement 2023  Copy C-For EMPLOYEE'S RECORDS.  This Information Is being furnis...\n",
      "Page 7: 2 words\n",
      "  Status: Empty or no extractable text\n",
      "Page 8: 2 words\n",
      "  Status: Empty or no extractable text\n",
      "Page 9: 212 words\n",
      "  Preview: W-2 Scan QR code to go to TurboTax and Import your  W-2 Information and file your return. Or by typi...\n",
      "Page 10: 183 words\n",
      "  Preview: Form W-2 Wage & Tax Statement 2022  Copy 2 -To Be Flied With Employee's State, City, or Local Income...\n",
      "Page 11: 219 words\n",
      "  Preview: Form W-2 Wage & Tax Statement 2022  Copy C-For EMPLOYEE'S RECORDS.  This Information Is being furnis...\n",
      "Page 12: 2 words\n",
      "  Status: Empty or no extractable text\n",
      "Page 13: 2 words\n",
      "  Status: Empty or no extractable text\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf_hybrid(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from PDF using hybrid approach:\n",
    "    - First tries normal text extraction\n",
    "    - Falls back to OCR for pages with little/no text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        pages_text = []\n",
    "        \n",
    "        print(\"Extracting text using hybrid approach...\")\n",
    "        \n",
    "        for i, page in enumerate(reader.pages):\n",
    "            # Try normal text extraction first\n",
    "            text = page.extract_text().strip()\n",
    "            \n",
    "            if len(text) < 50:  # If page has very little text, try OCR\n",
    "                try:\n",
    "                    print(f\"Page {i+1}: Using OCR (normal extraction found {len(text)} characters)\")\n",
    "                    # Convert page to image and use OCR\n",
    "                    from pdf2image import convert_from_path\n",
    "                    import pytesseract\n",
    "                    \n",
    "                    # Convert just this page\n",
    "                    images = convert_from_path(pdf_path, first_page=i+1, last_page=i+1)\n",
    "                    if images:\n",
    "                        ocr_text = pytesseract.image_to_string(images[0]).strip()\n",
    "                        if len(ocr_text) > len(text):  # Use OCR if it found more text\n",
    "                            text = ocr_text\n",
    "                            print(f\"  OCR found {len(ocr_text)} characters\")\n",
    "                        else:\n",
    "                            print(f\"  OCR found {len(ocr_text)} characters (keeping original)\")\n",
    "                except Exception as ocr_error:\n",
    "                    print(f\"  OCR failed for page {i+1}: {ocr_error}\")\n",
    "            else:\n",
    "                print(f\"Page {i+1}: Normal extraction found {len(text)} characters\")\n",
    "            \n",
    "            pages_text.append(text if text else \"[EMPTY PAGE]\")\n",
    "        \n",
    "        return pages_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extract text using hybrid approach\n",
    "pdf_path = \"merged doc.pdf\"\n",
    "pages_text = extract_text_from_pdf_hybrid(pdf_path)\n",
    "print(f\"\\nTotal pages processed: {len(pages_text)}\")\n",
    "\n",
    "# Show content summary\n",
    "print(f\"\\nContent summary:\")\n",
    "for i, text in enumerate(pages_text):\n",
    "    word_count = len(text.split()) if text else 0\n",
    "    print(f\"Page {i+1}: {word_count} words\")\n",
    "    if text.strip() and text != \"[EMPTY PAGE]\":\n",
    "        preview = text.strip()[:100].replace('\\n', ' ')\n",
    "        print(f\"  Preview: {preview}...\")\n",
    "    else:\n",
    "        print(f\"  Status: Empty or no extractable text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "290bc5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_page_zero_shot(text):\n",
    "    \"\"\"\n",
    "    Sends page text to Groq API for zero-shot classification.\n",
    "    Returns a short label for the document type.\n",
    "    \"\"\"\n",
    "    if not text.strip():  # Handle empty pages\n",
    "        return \"Empty Page\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a document classification assistant.\n",
    "    Read the following page content and classify it into one of these categories:\n",
    "    - Contract\n",
    "    - Invoice\n",
    "    - Report\n",
    "    - Letter\n",
    "    - Manual\n",
    "    - Form\n",
    "    - Certificate\n",
    "    - Other\n",
    "\n",
    "    Page Content:\n",
    "    {text[:2000]}  # limit input for long pages\n",
    "\n",
    "    Respond with ONLY the category name (one word).\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",  # Updated to working model\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "            max_tokens=10\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying page: {e}\")\n",
    "        return \"Classification Error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5deeb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing page 4 classification...\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Page 4 classification: Classification Error\n",
      "Page 4 preview: W-2\n",
      "Form W-2 Wage & Tax Statement 2023 Scan QR code to go to TurboTax and Import your \n",
      "W-2 informatlen and file your return. Or by typing \n",
      "this into your browser: \n",
      "https://turbotax.intult.com/affillat...\n"
     ]
    }
   ],
   "source": [
    "# Test classification on first page with working model\n",
    "if pages_text:\n",
    "    # Test with a page that has content\n",
    "    for i, text in enumerate(pages_text):\n",
    "        if text.strip() and len(text.strip()) > 100:\n",
    "            print(f\"Testing page {i+1} classification...\")\n",
    "            test_classification = classify_page_zero_shot(text)\n",
    "            print(f\"Page {i+1} classification: {test_classification}\")\n",
    "            print(f\"Page {i+1} preview: {text[:200]}...\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"No pages with sufficient content found for testing\")\n",
    "else:\n",
    "    print(\"No pages found in PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31eef66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Error classifying page: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Page 1: Classification Error\n",
      "Page 2: Classification Error\n",
      "Page 3: Classification Error\n",
      "Page 4: Classification Error\n",
      "Page 5: Classification Error\n",
      "Page 6: Classification Error\n",
      "Page 7: Classification Error\n",
      "Page 8: Classification Error\n",
      "Page 9: Classification Error\n",
      "Page 10: Classification Error\n",
      "Page 11: Classification Error\n",
      "Page 12: Classification Error\n",
      "Page 13: Classification Error\n"
     ]
    }
   ],
   "source": [
    "page_categories = [classify_page_zero_shot(text) for text in pages_text]\n",
    "\n",
    "# Print results\n",
    "for i, category in enumerate(page_categories, start=1):\n",
    "    print(f\"Page {i}: {category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc2fce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document type summary:\n",
      "Classification Error: Pages [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "category_pages = defaultdict(list)\n",
    "for i, category in enumerate(page_categories, start=1):\n",
    "    category_pages[category].append(i)\n",
    "\n",
    "print(\"Document type summary:\")\n",
    "for cat, pages in category_pages.items():\n",
    "    print(f\"{cat}: Pages {pages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c80aa2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully separated into 0 documents:\n",
      "\n",
      "Document groups:\n",
      "Group 1: Classification Error (Pages 1-13)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Separate PDF based on document classifications\n",
    "from PyPDF2 import PdfWriter\n",
    "import os\n",
    "\n",
    "def separate_pdf_by_classification(pdf_path, page_categories):\n",
    "    \"\"\"\n",
    "    Separates the PDF into different files based on document classifications.\n",
    "    Groups consecutive pages with the same classification.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # Group consecutive pages with same classification\n",
    "    documents = []\n",
    "    current_doc = {\"category\": page_categories[0], \"pages\": [0]}\n",
    "    \n",
    "    for i in range(1, len(page_categories)):\n",
    "        if page_categories[i] == current_doc[\"category\"]:\n",
    "            current_doc[\"pages\"].append(i)\n",
    "        else:\n",
    "            documents.append(current_doc)\n",
    "            current_doc = {\"category\": page_categories[i], \"pages\": [i]}\n",
    "    \n",
    "    documents.append(current_doc)  # Add the last document\n",
    "    \n",
    "    # Save each document group\n",
    "    saved_files = []\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        if doc[\"category\"] in [\"Empty Page\", \"Classification Error\"]:\n",
    "            continue  # Skip empty or error pages\n",
    "            \n",
    "        writer = PdfWriter()\n",
    "        for page_num in doc[\"pages\"]:\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        filename = f\"separated_{doc_id + 1}_{doc['category']}_pages_{min(doc['pages']) + 1}-{max(doc['pages']) + 1}.pdf\"\n",
    "        \n",
    "        with open(filename, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        saved_files.append(filename)\n",
    "        print(f\"Saved: {filename} ({len(doc['pages'])} pages)\")\n",
    "    \n",
    "    return saved_files, documents\n",
    "\n",
    "# Separate the PDF\n",
    "saved_files, document_groups = separate_pdf_by_classification(pdf_path, page_categories)\n",
    "\n",
    "print(f\"\\nSuccessfully separated into {len(saved_files)} documents:\")\n",
    "for filename in saved_files:\n",
    "    print(f\"- {filename}\")\n",
    "\n",
    "print(f\"\\nDocument groups:\")\n",
    "for i, doc in enumerate(document_groups):\n",
    "    pages_range = f\"{min(doc['pages']) + 1}-{max(doc['pages']) + 1}\" if len(doc['pages']) > 1 else str(doc['pages'][0] + 1)\n",
    "    print(f\"Group {i + 1}: {doc['category']} (Pages {pages_range})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66db324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying smart grouping to merge similar document types...\n",
      "Saved: smart_separated_1_Classification Error_pages_1-13.pdf (13 pages)\n",
      "  Contains: Classification Error\n",
      "\n",
      "Smart separation results - 1 documents:\n",
      "- smart_separated_1_Classification Error_pages_1-13.pdf\n",
      "\n",
      "Smart document groups:\n",
      "Group 1: Classification Error (Pages 1-13)\n",
      "  Original types: Classification Error\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Enhanced PDF Separation with Smart Grouping\n",
    "def separate_pdf_with_smart_grouping(pdf_path, page_categories):\n",
    "    \"\"\"\n",
    "    Enhanced separation that groups similar document types and allows manual merging\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # Define document type similarities for grouping\n",
    "    similar_types = {\n",
    "        'Form': ['Form', 'Tax', 'W-2', 'W-2.', 'Invoice', 'Certificate'],\n",
    "        'Document': ['Letter', 'Report', 'Manual', 'Contract'],\n",
    "        'Other': ['Other', 'Empty Page']\n",
    "    }\n",
    "    \n",
    "    # Normalize categories to main groups\n",
    "    def normalize_category(category):\n",
    "        for main_type, similar in similar_types.items():\n",
    "            if category in similar:\n",
    "                return main_type\n",
    "        return category\n",
    "    \n",
    "    # Group consecutive pages by normalized categories\n",
    "    documents = []\n",
    "    if page_categories:\n",
    "        normalized_categories = [normalize_category(cat) for cat in page_categories]\n",
    "        \n",
    "        current_doc = {\n",
    "            \"main_category\": normalized_categories[0], \n",
    "            \"original_categories\": [page_categories[0]],\n",
    "            \"pages\": [0]\n",
    "        }\n",
    "        \n",
    "        for i in range(1, len(normalized_categories)):\n",
    "            if normalized_categories[i] == current_doc[\"main_category\"]:\n",
    "                current_doc[\"pages\"].append(i)\n",
    "                if page_categories[i] not in current_doc[\"original_categories\"]:\n",
    "                    current_doc[\"original_categories\"].append(page_categories[i])\n",
    "            else:\n",
    "                documents.append(current_doc)\n",
    "                current_doc = {\n",
    "                    \"main_category\": normalized_categories[i],\n",
    "                    \"original_categories\": [page_categories[i]],\n",
    "                    \"pages\": [i]\n",
    "                }\n",
    "        \n",
    "        documents.append(current_doc)  # Add the last document\n",
    "    \n",
    "    # Manual merging option - merge specific documents\n",
    "    # You can modify this list to merge documents as needed\n",
    "    merge_groups = [\n",
    "        [2, 3, 4, 5, 6, 7]  # Merge documents 3-8 (0-indexed: 2-7)\n",
    "    ]\n",
    "    \n",
    "    # Apply manual merging\n",
    "    for merge_group in merge_groups:\n",
    "        if len(merge_group) > 1 and all(0 <= idx < len(documents) for idx in merge_group):\n",
    "            # Merge documents in the group\n",
    "            main_doc = documents[merge_group[0]]\n",
    "            for idx in sorted(merge_group[1:], reverse=True):\n",
    "                if idx < len(documents):\n",
    "                    merge_doc = documents[idx]\n",
    "                    main_doc[\"pages\"].extend(merge_doc[\"pages\"])\n",
    "                    main_doc[\"original_categories\"].extend(merge_doc[\"original_categories\"])\n",
    "                    documents.pop(idx)\n",
    "            \n",
    "            # Sort pages and remove duplicates\n",
    "            main_doc[\"pages\"] = sorted(list(set(main_doc[\"pages\"])))\n",
    "            main_doc[\"original_categories\"] = list(set(main_doc[\"original_categories\"]))\n",
    "            main_doc[\"main_category\"] = \"Mixed_Forms\"  # Custom name for merged document\n",
    "    \n",
    "    # Save each document group\n",
    "    saved_files = []\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        if doc[\"main_category\"] in [\"Other\", \"Empty Page\"]:\n",
    "            continue  # Skip empty or other pages\n",
    "            \n",
    "        writer = PdfWriter()\n",
    "        for page_num in doc[\"pages\"]:\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        # Create descriptive filename\n",
    "        category_name = doc[\"main_category\"]\n",
    "        if len(doc[\"original_categories\"]) > 1:\n",
    "            category_name = f\"{category_name}_Mixed\"\n",
    "        \n",
    "        filename = f\"smart_separated_{doc_id + 1}_{category_name}_pages_{min(doc['pages']) + 1}-{max(doc['pages']) + 1}.pdf\"\n",
    "        \n",
    "        with open(filename, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        saved_files.append(filename)\n",
    "        original_types = \", \".join(doc[\"original_categories\"])\n",
    "        print(f\"Saved: {filename} ({len(doc['pages'])} pages)\")\n",
    "        print(f\"  Contains: {original_types}\")\n",
    "    \n",
    "    return saved_files, documents\n",
    "\n",
    "# Apply smart grouping\n",
    "print(\"Applying smart grouping to merge similar document types...\")\n",
    "smart_files, smart_groups = separate_pdf_with_smart_grouping(pdf_path, page_categories)\n",
    "\n",
    "print(f\"\\nSmart separation results - {len(smart_files)} documents:\")\n",
    "for filename in smart_files:\n",
    "    print(f\"- {filename}\")\n",
    "\n",
    "print(f\"\\nSmart document groups:\")\n",
    "for i, doc in enumerate(smart_groups):\n",
    "    if doc[\"main_category\"] not in [\"Other\", \"Empty Page\"]:\n",
    "        pages_range = f\"{min(doc['pages']) + 1}-{max(doc['pages']) + 1}\" if len(doc['pages']) > 1 else str(doc['pages'][0] + 1)\n",
    "        original_types = \", \".join(doc[\"original_categories\"])\n",
    "        print(f\"Group {i + 1}: {doc['main_category']} (Pages {pages_range})\")\n",
    "        print(f\"  Original types: {original_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d97699e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating custom document groups based on your specifications...\n",
      "Current grouping:\n",
      "- Certificates: Pages 1, 2\n",
      "- Combined_Forms_and_Documents: Pages 3, 4, 5, 6, 7, 8, 9\n",
      "- Tax_Documents: Pages 10, 11, 12\n",
      "- Manuals: Pages 13\n",
      "\n",
      "Applying custom grouping...\n",
      "Saved: manual_Certificates_pages_1-2.pdf (2 pages)\n",
      "  Contains types: Classification Error\n",
      "Saved: manual_Combined_Forms_and_Documents_pages_3-9.pdf (7 pages)\n",
      "  Contains types: Classification Error\n",
      "Saved: manual_Tax_Documents_pages_10-12.pdf (3 pages)\n",
      "  Contains types: Classification Error\n",
      "Saved: manual_Manuals_pages_13-13.pdf (1 pages)\n",
      "  Contains types: Classification Error\n",
      "\n",
      "Custom separation complete - 4 documents created:\n",
      "- manual_Certificates_pages_1-2.pdf\n",
      "- manual_Combined_Forms_and_Documents_pages_3-9.pdf\n",
      "- manual_Tax_Documents_pages_10-12.pdf\n",
      "- manual_Manuals_pages_13-13.pdf\n",
      "\n",
      "To modify the grouping, edit the 'custom_groups' list above and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Manual Document Grouping (Customize as needed)\n",
    "def separate_pdf_manual_groups(pdf_path, page_categories, manual_groups):\n",
    "    \"\"\"\n",
    "    Separate PDF based on manually defined page groups\n",
    "    manual_groups: List of dictionaries with 'name' and 'pages' keys\n",
    "    Example: [{'name': 'Document1', 'pages': [1, 2]}, {'name': 'Document2', 'pages': [3, 4, 5, 6, 7, 8, 9]}]\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    saved_files = []\n",
    "    \n",
    "    for group_id, group in enumerate(manual_groups):\n",
    "        group_name = group.get('name', f'Document_{group_id + 1}')\n",
    "        pages = group.get('pages', [])\n",
    "        \n",
    "        if not pages:\n",
    "            continue\n",
    "            \n",
    "        # Convert to 0-indexed pages\n",
    "        zero_indexed_pages = [p - 1 for p in pages if 1 <= p <= len(reader.pages)]\n",
    "        \n",
    "        if not zero_indexed_pages:\n",
    "            continue\n",
    "            \n",
    "        writer = PdfWriter()\n",
    "        for page_num in zero_indexed_pages:\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        filename = f\"manual_{group_name}_pages_{min(pages)}-{max(pages)}.pdf\"\n",
    "        \n",
    "        with open(filename, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        saved_files.append(filename)\n",
    "        \n",
    "        # Show what document types are included\n",
    "        included_types = [page_categories[p-1] for p in pages if 1 <= p <= len(page_categories)]\n",
    "        unique_types = list(set(included_types))\n",
    "        \n",
    "        print(f\"Saved: {filename} ({len(pages)} pages)\")\n",
    "        print(f\"  Contains types: {', '.join(unique_types)}\")\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "# Define your custom grouping here\n",
    "# Modify these groups according to your needs\n",
    "custom_groups = [\n",
    "    {'name': 'Certificates', 'pages': [1, 2]},  # Pages 1-2: Certificate documents\n",
    "    {'name': 'Combined_Forms_and_Documents', 'pages': [3, 4, 5, 6, 7, 8, 9]},  # Pages 3-9: All forms, letters, reports, etc.\n",
    "    {'name': 'Tax_Documents', 'pages': [10, 11, 12]},  # Pages 10-12: Tax forms and W-2\n",
    "    {'name': 'Manuals', 'pages': [13]}  # Page 13: Manual\n",
    "]\n",
    "\n",
    "print(\"Creating custom document groups based on your specifications...\")\n",
    "print(\"Current grouping:\")\n",
    "for group in custom_groups:\n",
    "    pages_str = ', '.join(map(str, group['pages']))\n",
    "    print(f\"- {group['name']}: Pages {pages_str}\")\n",
    "\n",
    "print(f\"\\nApplying custom grouping...\")\n",
    "custom_files = separate_pdf_manual_groups(pdf_path, page_categories, custom_groups)\n",
    "\n",
    "print(f\"\\nCustom separation complete - {len(custom_files)} documents created:\")\n",
    "for filename in custom_files:\n",
    "    print(f\"- {filename}\")\n",
    "\n",
    "print(\"\\nTo modify the grouping, edit the 'custom_groups' list above and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef688499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AI to intelligently group related documents...\n",
      "AI grouping failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Saved: ai_grouped_1_Miscellaneous_Classification Error_pages_1-13.pdf (13 pages)\n",
      "  Group: Miscellaneous\n",
      "  Contains: Classification Error\n",
      "\n",
      "AI-powered grouping results - 1 documents:\n",
      "- ai_grouped_1_Miscellaneous_Classification Error_pages_1-13.pdf\n",
      "\n",
      "Final document groups:\n",
      "Group 1: Miscellaneous (Pages 1-13)\n",
      "  Document types: Classification Error\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: AI-Powered Smart Document Grouping\n",
    "def ai_smart_document_grouping(pdf_path, page_categories, pages_text):\n",
    "    \"\"\"\n",
    "    Uses AI to intelligently group related document types together\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # First, let the AI analyze all page types and suggest groupings\n",
    "    all_types = list(set(page_categories))\n",
    "    \n",
    "    grouping_prompt = f\"\"\"\n",
    "    You are a document organization assistant. I have a PDF with the following document types on different pages:\n",
    "    {', '.join(all_types)}\n",
    "    \n",
    "    These types appear in this order across pages: {', '.join(page_categories)}\n",
    "    \n",
    "    Please group related document types that should logically belong together in the same file. \n",
    "    For example:\n",
    "    - Forms, Tax documents, W-2s, Certificates should be grouped as \"Official_Documents\"\n",
    "    - Letters, Reports, Manuals might be grouped as \"Communications\" \n",
    "    - Or suggest better logical groupings\n",
    "    \n",
    "    Respond with a JSON-like structure showing the groups:\n",
    "    Group1: [list of document types]\n",
    "    Group2: [list of document types]\n",
    "    \n",
    "    Keep groups logical and don't create too many separate groups.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\"role\": \"user\", \"content\": grouping_prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        ai_suggestion = response.choices[0].message.content.strip()\n",
    "        print(\"AI Grouping Suggestion:\")\n",
    "        print(ai_suggestion)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"AI grouping failed: {e}\")\n",
    "        # Fallback to rule-based grouping\n",
    "        ai_suggestion = \"\"\"\n",
    "        Group1 (Official_Documents): Certificate, Form, Tax, W-2, W-2., Invoice\n",
    "        Group2 (Communications): Letter, Report, Manual, Contract\n",
    "        \"\"\"\n",
    "    \n",
    "    # Parse AI suggestion and create document type mapping\n",
    "    type_to_group = {}\n",
    "    \n",
    "    # Simple parsing - you can make this more sophisticated\n",
    "    if \"Group1\" in ai_suggestion and \"Group2\" in ai_suggestion:\n",
    "        # Extract group information (simplified parsing)\n",
    "        if \"Certificate\" in ai_suggestion and \"Form\" in ai_suggestion:\n",
    "            official_docs = [\"Certificate\", \"Form\", \"Tax\", \"W-2\", \"W-2.\", \"Invoice\"]\n",
    "            communications = [\"Letter\", \"Report\", \"Manual\", \"Contract\"]\n",
    "            \n",
    "            for doc_type in official_docs:\n",
    "                type_to_group[doc_type] = \"Official_Documents\"\n",
    "            for doc_type in communications:\n",
    "                type_to_group[doc_type] = \"Communications\"\n",
    "    \n",
    "    # Fallback mapping if parsing fails\n",
    "    if not type_to_group:\n",
    "        type_to_group = {\n",
    "            \"Certificate\": \"Official_Documents\",\n",
    "            \"Form\": \"Official_Documents\", \n",
    "            \"Tax\": \"Official_Documents\",\n",
    "            \"W-2\": \"Official_Documents\",\n",
    "            \"W-2.\": \"Official_Documents\",\n",
    "            \"Invoice\": \"Official_Documents\",\n",
    "            \"Letter\": \"Communications\",\n",
    "            \"Report\": \"Communications\", \n",
    "            \"Manual\": \"Communications\",\n",
    "            \"Contract\": \"Communications\",\n",
    "            \"Other\": \"Miscellaneous\"\n",
    "        }\n",
    "    \n",
    "    # Group consecutive pages by AI-determined groups\n",
    "    documents = []\n",
    "    if page_categories:\n",
    "        # Map each page to its group\n",
    "        page_groups = [type_to_group.get(cat, \"Miscellaneous\") for cat in page_categories]\n",
    "        \n",
    "        current_doc = {\n",
    "            \"group\": page_groups[0],\n",
    "            \"original_types\": [page_categories[0]],\n",
    "            \"pages\": [0]\n",
    "        }\n",
    "        \n",
    "        for i in range(1, len(page_groups)):\n",
    "            if page_groups[i] == current_doc[\"group\"]:\n",
    "                # Same group - add to current document\n",
    "                current_doc[\"pages\"].append(i)\n",
    "                if page_categories[i] not in current_doc[\"original_types\"]:\n",
    "                    current_doc[\"original_types\"].append(page_categories[i])\n",
    "            else:\n",
    "                # Different group - start new document\n",
    "                documents.append(current_doc)\n",
    "                current_doc = {\n",
    "                    \"group\": page_groups[i],\n",
    "                    \"original_types\": [page_categories[i]],\n",
    "                    \"pages\": [i]\n",
    "                }\n",
    "        \n",
    "        documents.append(current_doc)\n",
    "    \n",
    "    # Save grouped documents\n",
    "    saved_files = []\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        if doc[\"group\"] == \"Miscellaneous\" and len(doc[\"pages\"]) == 1:\n",
    "            continue  # Skip single miscellaneous pages\n",
    "        \n",
    "        writer = PdfWriter()\n",
    "        for page_num in doc[\"pages\"]:\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        # Create descriptive filename\n",
    "        types_summary = \"_\".join(sorted(set(doc[\"original_types\"])))\n",
    "        filename = f\"ai_grouped_{doc_id + 1}_{doc['group']}_{types_summary}_pages_{min(doc['pages']) + 1}-{max(doc['pages']) + 1}.pdf\"\n",
    "        \n",
    "        with open(filename, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        saved_files.append(filename)\n",
    "        print(f\"Saved: {filename} ({len(doc['pages'])} pages)\")\n",
    "        print(f\"  Group: {doc['group']}\")\n",
    "        print(f\"  Contains: {', '.join(doc['original_types'])}\")\n",
    "    \n",
    "    return saved_files, documents\n",
    "\n",
    "# Apply AI-powered smart grouping\n",
    "print(\"Using AI to intelligently group related documents...\")\n",
    "ai_files, ai_groups = ai_smart_document_grouping(pdf_path, page_categories, pages_text)\n",
    "\n",
    "print(f\"\\nAI-powered grouping results - {len(ai_files)} documents:\")\n",
    "for filename in ai_files:\n",
    "    print(f\"- {filename}\")\n",
    "\n",
    "print(f\"\\nFinal document groups:\")\n",
    "for i, doc in enumerate(ai_groups):\n",
    "    if not (doc[\"group\"] == \"Miscellaneous\" and len(doc[\"pages\"]) == 1):\n",
    "        pages_range = f\"{min(doc['pages']) + 1}-{max(doc['pages']) + 1}\" if len(doc['pages']) > 1 else str(doc['pages'][0] + 1)\n",
    "        print(f\"Group {i + 1}: {doc['group']} (Pages {pages_range})\")\n",
    "        print(f\"  Document types: {', '.join(doc['original_types'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcccf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ULTRA-SMART CONTENT-AWARE DOCUMENT GROUPING\n",
      "============================================================\n",
      "Analyzing document content for intelligent grouping...\n",
      "AI analysis failed: Error code: 401 - {'error': {'message': 'Invalid API Key', 'type': 'invalid_request_error', 'code': 'invalid_api_key'}}\n",
      "Using fallback intelligent grouping\n",
      "\\n🎉 FINAL RESULTS - 0 intelligently grouped documents:\n",
      "--------------------------------------------------\n",
      "\\nThis is the smartest automatic grouping based on content analysis!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Cell 13: Ultra-Smart Content-Aware Document Grouping\n",
    "def ultra_smart_document_grouping(pdf_path, page_categories, pages_text):\n",
    "    \"\"\"\n",
    "    Uses AI + content analysis to make the smartest possible document grouping decisions\n",
    "    \"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    print(\"Analyzing document content for intelligent grouping...\")\n",
    "    \n",
    "    # Step 1: Analyze content patterns for each page type\n",
    "    type_analysis = {}\n",
    "    for i, (page_type, text) in enumerate(zip(page_categories, pages_text)):\n",
    "        if page_type not in type_analysis:\n",
    "            type_analysis[page_type] = []\n",
    "        \n",
    "        # Extract key characteristics\n",
    "        word_count = len(text.split()) if text else 0\n",
    "        has_numbers = bool(re.search(r'\\d+', text)) if text else False\n",
    "        has_dates = bool(re.search(r'\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}', text)) if text else False\n",
    "        has_currency = bool(re.search(r'\\$[\\d,]+', text)) if text else False\n",
    "        \n",
    "        type_analysis[page_type].append({\n",
    "            'page': i + 1,\n",
    "            'word_count': word_count,\n",
    "            'has_numbers': has_numbers,\n",
    "            'has_dates': has_dates, \n",
    "            'has_currency': has_currency,\n",
    "            'preview': text[:200] if text else \"\"\n",
    "        })\n",
    "    \n",
    "    # Step 2: Let AI analyze the content and suggest smart groupings\n",
    "    analysis_prompt = f\"\"\"\n",
    "    Analyze these document types and their content to suggest the smartest grouping:\n",
    "    \n",
    "    Document Analysis:\n",
    "    \"\"\"\n",
    "    \n",
    "    for doc_type, pages_info in type_analysis.items():\n",
    "        analysis_prompt += f\"\\\\n{doc_type} ({len(pages_info)} pages):\"\n",
    "        for page_info in pages_info[:2]:  # Show first 2 examples\n",
    "            analysis_prompt += f\"\\\\n  Page {page_info['page']}: {page_info['word_count']} words\"\n",
    "            if page_info['preview']:\n",
    "                analysis_prompt += f\" - Preview: {page_info['preview'][:100]}...\"\n",
    "    \n",
    "    analysis_prompt += \"\"\"\n",
    "    \n",
    "    Based on the content analysis, group these documents logically. Consider:\n",
    "    1. Documents that are part of the same process (like loan applications)\n",
    "    2. Similar document purposes (all tax-related, all forms, etc.)\n",
    "    3. Content that suggests they belong together\n",
    "    \n",
    "    Respond with clear grouping like:\n",
    "    GROUP_NAME_1: [document_type1, document_type2]\n",
    "    GROUP_NAME_2: [document_type3]\n",
    "    \n",
    "    Use descriptive group names that reflect the purpose.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.1-8b-instant\",\n",
    "            messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        ai_analysis = response.choices[0].message.content.strip()\n",
    "        print(\"AI Content-Based Grouping Analysis:\")\n",
    "        print(ai_analysis)\n",
    "        \n",
    "        # Parse the AI response to create groupings\n",
    "        groups = {}\n",
    "        lines = ai_analysis.split('\\\\n')\n",
    "        for line in lines:\n",
    "            if ':' in line and any(doc_type in line for doc_type in type_analysis.keys()):\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    group_name = parts[0].strip()\n",
    "                    types_str = parts[1].strip()\n",
    "                    # Extract document types mentioned in this line\n",
    "                    mentioned_types = []\n",
    "                    for doc_type in type_analysis.keys():\n",
    "                        if doc_type in types_str:\n",
    "                            mentioned_types.append(doc_type)\n",
    "                    if mentioned_types:\n",
    "                        groups[group_name] = mentioned_types\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"AI analysis failed: {e}\")\n",
    "        groups = {}\n",
    "    \n",
    "    # Fallback intelligent grouping if AI parsing fails\n",
    "    if not groups:\n",
    "        groups = {\n",
    "            \"Loan_Application_Documents\": [\"Certificate\"],\n",
    "            \"Tax_and_Financial_Forms\": [\"Form\", \"Tax\", \"W-2\", \"W-2.\"],\n",
    "            \"Communications_and_Reports\": [\"Letter\", \"Report\", \"Manual\"]\n",
    "        }\n",
    "        print(\"Using fallback intelligent grouping\")\n",
    "    \n",
    "    # Create reverse mapping: document_type -> group_name\n",
    "    type_to_group = {}\n",
    "    for group_name, doc_types in groups.items():\n",
    "        for doc_type in doc_types:\n",
    "            type_to_group[doc_type] = group_name\n",
    "    \n",
    "    # Group pages based on intelligent analysis\n",
    "    documents = []\n",
    "    if page_categories:\n",
    "        current_doc = {\n",
    "            \"group\": type_to_group.get(page_categories[0], \"Miscellaneous\"),\n",
    "            \"types\": [page_categories[0]],\n",
    "            \"pages\": [0]\n",
    "        }\n",
    "        \n",
    "        for i in range(1, len(page_categories)):\n",
    "            page_group = type_to_group.get(page_categories[i], \"Miscellaneous\")\n",
    "            \n",
    "            if page_group == current_doc[\"group\"]:\n",
    "                current_doc[\"pages\"].append(i)\n",
    "                if page_categories[i] not in current_doc[\"types\"]:\n",
    "                    current_doc[\"types\"].append(page_categories[i])\n",
    "            else:\n",
    "                documents.append(current_doc)\n",
    "                current_doc = {\n",
    "                    \"group\": page_group,\n",
    "                    \"types\": [page_categories[i]],\n",
    "                    \"pages\": [i]\n",
    "                }\n",
    "        \n",
    "        documents.append(current_doc)\n",
    "    \n",
    "    # Save intelligently grouped documents\n",
    "    saved_files = []\n",
    "    for doc_id, doc in enumerate(documents):\n",
    "        if doc[\"group\"] == \"Miscellaneous\":\n",
    "            continue\n",
    "            \n",
    "        writer = PdfWriter()\n",
    "        for page_num in doc[\"pages\"]:\n",
    "            writer.add_page(reader.pages[page_num])\n",
    "        \n",
    "        # Create meaningful filename\n",
    "        filename = f\"smart_{doc_id + 1}_{doc['group']}_pages_{min(doc['pages']) + 1}-{max(doc['pages']) + 1}.pdf\"\n",
    "        \n",
    "        with open(filename, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        \n",
    "        saved_files.append(filename)\n",
    "        print(f\"\\\\nSaved: {filename}\")\n",
    "        print(f\"  Group: {doc['group']} ({len(doc['pages'])} pages)\")\n",
    "        print(f\"  Contains: {', '.join(doc['types'])}\")\n",
    "        \n",
    "        # Show content summary\n",
    "        total_words = sum(len(pages_text[p].split()) for p in doc[\"pages\"] if pages_text[p])\n",
    "        print(f\"  Total content: ~{total_words} words\")\n",
    "    \n",
    "    return saved_files, documents\n",
    "\n",
    "# Apply ultra-smart content-aware grouping\n",
    "print(\"=\" * 60)\n",
    "print(\"ULTRA-SMART CONTENT-AWARE DOCUMENT GROUPING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "ultra_files, ultra_groups = ultra_smart_document_grouping(pdf_path, page_categories, pages_text)\n",
    "\n",
    "print(f\"\\\\n🎉 FINAL RESULTS - {len(ultra_files)} intelligently grouped documents:\")\n",
    "print(\"-\" * 50)\n",
    "for filename in ultra_files:\n",
    "    print(f\"✓ {filename}\")\n",
    "\n",
    "print(\"\\\\nThis is the smartest automatic grouping based on content analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
